import os
import json
import numpy as np
import os
import json
import numpy as np
# import google.generativeai as genai  <-- [Mod] Lazy importë¡œ ë³€ê²½
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# ë¡œì»¬ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì´ ìˆë‹¤ë©´)
# 1. í˜„ì¬ í´ë”(poc) í™•ì¸
# 2. í˜•ì œ í´ë”(backend) í™•ì¸
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_env_path = os.path.join(current_dir, "..", "backend", ".env")

if os.path.exists(backend_env_path):
    load_dotenv(backend_env_path)
    # print(f"âœ… Loaded .env from: {os.path.abspath(backend_env_path)}")
else:
    load_dotenv() # ê¸°ë³¸ê°’: í˜„ì¬ í´ë”

# ==========================================
# âš™ï¸ ì„¤ì • (Configuration)
# ==========================================
# Test 1 ì‚¬ìš© (Gemini) -> get_gemini_embedding í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì„¤ì •í•¨

# Test 2 ì‚¬ìš© (Local)
# ë‹¤êµ­ì–´(í•œêµ­ì–´ í¬í•¨) ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ê²½ëŸ‰í™” ëª¨ë¸
LOCAL_MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"
_local_model_instance = None # Lazy Loading

# ë°ì´í„° ê²½ë¡œ
DATA_PATH = os.path.join(os.path.dirname(__file__), "data", "products.json")


# ==========================================
# ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Utilities)
# ==========================================
def load_data():
    """Dummy JSON ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    with open(DATA_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def get_gemini_embedding(text, task_type="retrieval_document"):
    """[Test 1] Gemini APIë¥¼ ì‚¬ìš© (ìˆ˜ì •ì „)"""
    # Lazy Import: í•¨ìˆ˜ê°€ í˜¸ì¶œë  ë•Œë§Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
    import google.generativeai as genai
    
    api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    genai.configure(api_key=api_key)

    result = genai.embed_content(
        model="models/text-embedding-004",
        content=text,
        task_type=task_type
    )
    return result['embedding']

def get_local_embedding(text):
    """[Test 2] Local Sentence-BERT ì‚¬ìš© (ìˆ˜ì •í›„)"""
    global _local_model_instance
    if _local_model_instance is None:
        print(f"ğŸ“¥ ë¡œì»¬ ëª¨ë¸({LOCAL_MODEL_NAME}) ë¡œë”© ì¤‘... (ìµœì´ˆ 1íšŒë§Œ ëŠë¦¼)")
        _local_model_instance = SentenceTransformer(LOCAL_MODEL_NAME)
    
    # SentenceTransformerëŠ” ë°”ë¡œ embedding ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    return _local_model_instance.encode(text).tolist()

def cosine_similarity(v1, v2):
    """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    return dot_product / (norm_v1 * norm_v2)

def vector_search(query, all_products, top_k=10, threshold=0.0, use_local_model=True):
    """
    ê¸°ë³¸ì ì¸ ë²¡í„° ê²€ìƒ‰ ë¡œì§.
    use_local_model=Trueì´ë©´ ë¡œì»¬ ëª¨ë¸(Test 2) ì‚¬ìš©.
    """
    # 1. ì¿¼ë¦¬ ì„ë² ë”©
    if use_local_model:
        query_vec = get_local_embedding(query)
    else:
        query_vec = get_gemini_embedding(query, task_type="retrieval_query")
    
    scored_products = []
    
    for product in all_products:
        # ìºì‹± ì²˜ë¦¬ (í‚¤ë¥¼ ë¶„ë¦¬í•´ì„œ ì €ì¥)
        emb_key = "embedding_local" if use_local_model else "embedding_gemini"
        
        if emb_key not in product:
            # "ì´ë¦„ + ì„¤ëª… + ì¹´í…Œê³ ë¦¬"ë¥¼ í•©ì³ì„œ ì„ë² ë”©
            text_to_embed = f"{product['name']} {product['desc']} {product['category']}"
            if use_local_model:
                product[emb_key] = get_local_embedding(text_to_embed)
            else:
                product[emb_key] = get_gemini_embedding(text_to_embed)
            
        params_vec = product[emb_key]
        score = cosine_similarity(query_vec, params_vec)
        
        if score >= threshold:
            product_with_score = product.copy()
            product_with_score["score"] = score
            # ê²°ê³¼ ì¶œë ¥ ì‹œ ê¸´ ë²¡í„° ì •ë³´ëŠ” ì‚­ì œ
            if "embedding_local" in product_with_score: del product_with_score["embedding_local"]
            if "embedding_gemini" in product_with_score: del product_with_score["embedding_gemini"]
            
            scored_products.append(product_with_score)
    
    scored_products.sort(key=lambda x: x["score"], reverse=True)
    return scored_products[:top_k]


# ==========================================
# ğŸ§ª ì‹¤í—˜ Run Functions
# ==========================================

def run_grid_search(products, mode="local"):
    """
    mode='gemini' -> Test 1 (ìˆ˜ì •ì „)
    mode='local'  -> Test 2 (ìˆ˜ì •í›„)
    """
    is_local = (mode == "local")
    title = "[Test 2] ìˆ˜ì •í›„: Local Model" if is_local else "[Test 1] ìˆ˜ì •ì „: Gemini API"
    
    print("\n" + "="*60)
    print(f"ğŸ§ª {title} - Grid Search")
    print("="*60)
    
    query = "ìš•ì‹¤ë§¤íŠ¸"
    
    # âœ… Ground Truth (ì •ë‹µì§€) - ì—„ê²© ê¸°ì¤€
    ground_truth_ids = {1, 5, 9}
    
    print(f"ğŸ” ê²€ìƒ‰ì–´: '{query}'")
    print(f"ğŸ¯ ì •ë‹µì…‹(Ground Truth): ID {list(ground_truth_ids)}")
    
    k_candidates = [3, 5, 7, 10]
    thr_candidates = [0.40, 0.50, 0.60, 0.70] if is_local else [0.60, 0.70, 0.80] 
    # ë¡œì»¬ ëª¨ë¸ì€ ì ìˆ˜ ë¶„í¬ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë²”ìœ„ë¥¼ ì¡°ê¸ˆ ë‚®ì¶¤
    
    best_score = 0
    best_params = {}
    
    print(f"\n{'K':<4} | {'Thr':<6} | {'Found':<5} | {'Prec(ì •í™•)':<10} | {'Rec(ì¬í˜„)':<10} | {'F1-Score':<10} | {'íŒì •'}")
    print("-" * 80)
    
    for k in k_candidates:
        for thr in thr_candidates:
            # ê²€ìƒ‰ ìˆ˜í–‰
            results = vector_search(query, products, top_k=k, threshold=thr, use_local_model=is_local)
            
            retrieved_ids = set([item['id'] for item in results])
            
            # Metric ê³„ì‚°
            if len(retrieved_ids) == 0:
                precision = 0.0
            else:
                precision = len(retrieved_ids.intersection(ground_truth_ids)) / len(retrieved_ids)
                
            if len(ground_truth_ids) == 0:
                recall = 0.0
            else:
                recall = len(retrieved_ids.intersection(ground_truth_ids)) / len(ground_truth_ids)
            
            if (precision + recall) == 0:
                f1_score = 0.0
            else:
                f1_score = 2 * (precision * recall) / (precision + recall)
            
            verdict = ""
            if f1_score >= 0.8: verdict = "ğŸ† Excellent"
            elif f1_score >= 0.6: verdict = "âœ… Good"
            else: verdict = "âŒ Fail"
            
            if f1_score > best_score:
                best_score = f1_score
                best_params = {"k": k, "thr": thr}
            
            print(f"{k:<4} | {thr:<6} | {len(results):<5} | {precision:.3f}      | {recall:.3f}      | {f1_score:.3f}      | {verdict}")

    print("-" * 80)
    print(f"\nğŸ‰ [{mode} ëª¨ë“œ ê²°ê³¼] Best F1: {best_score:.3f}")
    if best_params:
        print(f"ğŸ‘‰ Recommended: Top-K={best_params['k']}, Threshold={best_params['thr']}")
    
    # [DEBUG] ìƒì„¸ ë­í‚¹ í™•ì¸ (Why?)
    print("\nğŸ•µï¸ [DEBUG] Ranking Check")
    full_results = vector_search(query, products, top_k=len(products), threshold=0.0, use_local_model=is_local)
    
    print(f"ğŸ” ì •ë‹µ ìƒí’ˆ ìˆœìœ„:")
    for rank, item in enumerate(full_results):
        if item['id'] in ground_truth_ids:
            print(f" - #{rank+1}ìœ„: [{item['score']:.4f}] {item['name']}")
            
    print(f"\nğŸ” Top 5 ì˜¤ë‹µ(Noise) í™•ì¸:")
    count = 0
    for rank, item in enumerate(full_results):
        if item['id'] not in ground_truth_ids:
            print(f" - #{rank+1}ìœ„: [{item['score']:.4f}] {item['name']} ({item['category']})")
            count += 1
            if count >= 5: break


# ==========================================
# ğŸ”® Future Steps (Issue 2, 3)
# í˜„ì¬ Sprint 1ì—ì„œëŠ” Issue 1(Retrieval)ì— ì§‘ì¤‘í•˜ê³  ìˆìŠµë‹ˆë‹¤.
# ì•„ë˜ í•¨ìˆ˜ë“¤ì€ Issue 1ì´ í•´ê²°ëœ í›„(Next Sprint), ìˆœì°¨ì ìœ¼ë¡œ í™œì„±í™”í•˜ì—¬ ì‹¤í—˜í•  ì˜ˆì •ì…ë‹ˆë‹¤.
# ==========================================

def experiment_llm_reranking(products):
    """
    [Issue 2] LLM Re-ranking
    1ì°¨ ê²€ìƒ‰(Retrieval) ê²°ê³¼ì—ì„œ ë¬¸ë§¥ì ìœ¼ë¡œ ë§ì§€ ì•ŠëŠ” ìƒí’ˆì„ LLMì´ 2ì°¨ ê²€ìˆ˜í•˜ëŠ” ë¡œì§.
    """
    print("\n" + "="*80)
    print("ğŸ§ª [Test 3] LLM Re-ranking (Issue 2)")
    print("ëª©í‘œ: Top-7 ì•ˆì— ë“¤ì–´ì˜¨ ì •ë‹µ(#4, #5, #7)ì„ #1, #2, #3ìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ê¸°")
    print("="*80)

    # 1. 1ì°¨ ê²€ìƒ‰ (Retrieval) - Local Model, K=7
    query = "ìš•ì‹¤ë§¤íŠ¸"
    print(f"1ï¸âƒ£ 1ì°¨ ê²€ìƒ‰ ìˆ˜í–‰ (Query: '{query}', Model: Local, K=7)...")
    candidates = vector_search(query, products, top_k=7, threshold=0.0, use_local_model=True)
    
    print("\n[Before Re-ranking] 1ì°¨ ê²€ìƒ‰ ê²°ê³¼:")
    for i, item in enumerate(candidates):
        print(f" - Rank {i+1}: {item['name']} (Score: {item['score']:.4f})")

    # 2. LLMì—ê²Œ Re-ranking ìš”ì²­
    print("\n2ï¸âƒ£ Geminiì—ê²Œ Re-ranking ìš”ì²­ ì¤‘...")
    
    # Lazy Import & Config
    import google.generativeai as genai
    api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ GOOGLE_API_KEYê°€ ì—†ì–´ì„œ Skipí•©ë‹ˆë‹¤.")
        return
    genai.configure(api_key=api_key)
    
    # Prompt êµ¬ì„±
    candidate_text = ""
    for i, item in enumerate(candidates):
        candidate_text += f"ID {item['id']}: {item['name']} (ì„¤ëª…: {item['desc']})\n"
        
    prompt = f"""
    ë‹¹ì‹ ì€ ì‡¼í•‘ëª° ê²€ìƒ‰ í’ˆì§ˆ ê´€ë¦¬ìì…ë‹ˆë‹¤.
    ì‚¬ìš©ìê°€ "{query}"ë¼ê³  ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.
    ë‹¤ìŒì€ 1ì°¨ ê²€ìƒ‰ ê²°ê³¼ í›„ë³´ë“¤ì…ë‹ˆë‹¤.
    
    [í›„ë³´ ëª©ë¡]
    {candidate_text}
    
    [ì§€ì‹œì‚¬í•­]
    1. ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì˜ë„("{query}")ì— ê°€ì¥ ì í•©í•œ ìˆœì„œëŒ€ë¡œ ìƒí’ˆì„ ì¬ì •ë ¬í•˜ì„¸ìš”.
    2. "ìš•ì‹¤ì— ë°”ë‹¥ì— ê¹”ì•„ ì‚¬ìš©í•˜ëŠ” ë§¤íŠ¸"ê°€ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.
    3. ì´ë¦„ì— 'ë§¤íŠ¸'ê°€ ì—†ê±°ë‚˜ ìš©ë„ê°€ ë‹¤ë¥¸ ê²½ìš°(ì„ ë°˜, ì¹«ì†”ê½‚ì´ ë“±)ëŠ” í•˜ìœ„ê¶Œìœ¼ë¡œ ë‚´ë¦¬ì‹­ì‹œì˜¤.
    4. ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒ í¬ë§·ì— ë§ì¶° ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    [
        {{"id": ìƒí’ˆID, "rank": 1, "reason": "ì„ ì •ì´ìœ "}},
        ...
    ]
    """
    
    try:
        # gemini-2.0-flash (Verified Available)
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        
        # JSON íŒŒì‹± (ê°„ë‹¨í•œ ì²˜ë¦¬)
        import re
        json_match = re.search(r'\[.*\]', response.text, re.DOTALL)
        if json_match:
            rerank_results = json.loads(json_match.group(0))
            
            print("\nğŸ‰ [After Re-ranking] ìµœì¢… ê²°ê³¼:")
            for item in rerank_results:
                # ì›ë˜ ìƒí’ˆ ì •ë³´ ë§¤í•‘
                original_prod = next((p for p in candidates if p['id'] == item['id']), None)
                if original_prod:
                    mark = "âœ…" if item['id'] in {1, 5, 9} else "  "
                    print(f" - {mark} Rank {item['rank']}: {original_prod['name']} (Reason: {item.get('reason', '')})")
                    
        else:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨. ì‘ë‹µ ì›ë³¸:\n{response.text}")
            
    except Exception as e:
        print(f"âŒ Re-ranking ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

def classify_intent(query, categories):
    """
    [Intent Classifier]
    ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´(Query)ë¥¼ ë³´ê³  ê°€ì¥ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    # Lazy Import & Config
    import google.generativeai as genai
    api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key: return None
    genai.configure(api_key=api_key)
    
    cat_list_str = ", ".join(categories)
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì‡¼í•‘ëª° ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì˜ë„ ë¶„ë¥˜ê¸°(Intent Classifier)ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ê²€ìƒ‰ì–´: "{query}"
    
    [ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡]
    {cat_list_str}
    
    [ì§€ì‹œì‚¬í•­]
    1. ê²€ìƒ‰ì–´ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì¹´í…Œê³ ë¦¬ë¥¼ í•˜ë‚˜ë§Œ ì„ íƒí•˜ì„¸ìš”.
    2. ë‹µë³€ì€ ì¹´í…Œê³ ë¦¬ ëª…ì¹­ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”. (ì„¤ëª… ê¸ˆì§€)
    3. ëª©ë¡ì— ì—†ëŠ” ê²½ìš° ê°€ì¥ ê°€ê¹Œìš´ ê²ƒì„ ì„ íƒí•˜ê±°ë‚˜, ëª¨ë¥´ë©´ 'ê¸°íƒ€'ë¼ê³  í•˜ì„¸ìš”.
    """
    
    try:
        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        predicted_category = response.text.strip()
        
        # í›„ì²˜ë¦¬: ì´ìƒí•œ ë¬¸ì¥ ë¶€í˜¸ ì œê±°ë‚˜ ë§¤ì¹­ í™•ì¸
        for cat in categories:
            if cat in predicted_category:
                return cat
        return predicted_category
        
    except Exception as e:
        print(f"âŒ Intent Classification Error: {e}")
        return None

def experiment_category_filter(products):
    """
    [Issue 3] Dynamic Category Filter
    Staticí•œ "ìš•ì‹¤" í•„í„°ê°€ ì•„ë‹Œ, ê²€ìƒ‰ì–´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë¥¼ íŒë‹¨í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("ğŸ§ª [Test 4] Dynamic Category Filtering (Issue 3)")
    print("ëª©í‘œ: ì–´ë–¤ ê²€ìƒ‰ì–´(ìš•ì‹¤/ìš´ë™/ìº í•‘)ê°€ ë“¤ì–´ì™€ë„, ê·¸ì— ë§ëŠ” ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§í•˜ëŠ”ê°€?")
    print("="*80)
    
    # 1. ì „ì²´ ì¹´í…Œê³ ë¦¬ ìŠ¤í‚¤ë§ˆ ì¶”ì¶œ
    all_categories = set(p['category'] for p in products)
    print(f"ğŸ“‹ ê°ì§€ëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡: {all_categories}")
    
    # 2. ë‹¤ì¤‘ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_queries = [
        "ìš•ì‹¤ë§¤íŠ¸",         # Exp: ìš•ì‹¤
        "í™ˆíŠ¸ë ˆì´ë‹ ë§¤íŠ¸",    # Exp: ìš´ë™
        "ì•¼ì™¸ ë—ìë¦¬"        # Exp: ìº í•‘ or ìë™ì°¨
    ]
    
    for query in test_queries:
        print(f"\nğŸ” [Query]: '{query}'")
        
        # (1) ì˜ë„ ë¶„ë¥˜
        predicted = classify_intent(query, all_categories)
        if not predicted:
            print("âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨ (API Error)")
            continue
            
        print(f"ğŸ‘‰ AI íŒë‹¨ ì¹´í…Œê³ ë¦¬: '{predicted}'")
        
        # (2) í•„í„°ë§
        filtered_products = [p for p in products if p['category'] == predicted]
        print(f"ğŸ‘‰ í•„í„°ë§ ê²°ê³¼: {len(products)}ê°œ -> {len(filtered_products)}ê°œ")
        
        if not filtered_products:
            print("âŒ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ìƒí’ˆ ì—†ìŒ.")
            continue

        # (3) ê²€ìƒ‰ ìˆ˜í–‰ (Local Model)
        results = vector_search(query, filtered_products, top_k=3, threshold=0.0, use_local_model=True)
        
        print("ğŸ” Top-3 ê²€ìƒ‰ ê²°ê³¼:")
        for i, item in enumerate(results):
            print(f" - #{i+1}: {item['name']} (Category: {item['category']}, Score: {item['score']:.4f})")


# ==========================================
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# ==========================================
if __name__ == "__main__":
    print("ğŸ“¦ ë°ì´í„° ë¡œë”© ì¤‘...")
    products = load_data()
    print(f"âœ… {len(products)}ê°œ ìƒí’ˆ ë¡œë“œ ì™„ë£Œ.")
    
    """
    Test1: ìˆ˜ì •ì „, Gemini API(text-embedding-004) ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©.
    Test2: ìˆ˜ì •í›„, Local Model(MiniLM) ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©.
    """
    # run_grid_search(products, mode="gemini")
    
    print("\n" + "="*80)
    print("Test2:")
    print("ìˆ˜ì •í›„ (Local Model ì‚¬ìš©)")
    print("="*80)
    
    run_grid_search(products, mode="local")
    
    # Issue 2 Active
    experiment_llm_reranking(products)
    
    # Issue 3 Active
    experiment_category_filter(products)
